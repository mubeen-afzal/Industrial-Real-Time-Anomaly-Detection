ğŸ­ **Industrial Digital Twin: Predictive Maintenance Engine**

AWS Scheer Studentwerk Data Science Interview Project

"Bridging the gap between raw sensor data and actionable business insights."

---

## ğŸ“– Project Overview

This project demonstrates an End-to-End Industrial AI Pipeline designed to predict machine failures before they occur. It simulates a real-world Digital Twin for a water pump station using IoT sensor data.

Unlike standard "notebook-only" projects, this solution is architected as a production-grade software package, featuring:

- **SQL Data Engineering**: Simulating an ERP/Historian extraction process.
- **Unsupervised Deep Learning**: Using a PyTorch Autoencoder to detect anomalies without needing labeled failure data ("Golden Batch" training).
- **Real-Time Inference**: A decoupled Inference Engine that simulates live IoT streaming.
- **Interactive Dashboards**: Both a Python-based (Streamlit) Control Tower and a modern Web-based (FastAPI + HTML) dashboard.

---

## ğŸ“¸ Dashboard Preview

![Dashboard Preview](images/dashboard.png)

---

## ğŸ—ï¸ System Architecture

The project follows a modular Object-Oriented (OOP) design pattern to ensure scalability and maintainability.

![System Architecture](images/flowchart.png)

---

## ğŸš€ Key Features

### 1. Data Engineering (ETL) & SQL

- **Clean Architecture**: Raw CSV data is not used directly in training. It is first cleaned, processed, and loaded into a SQLite database to mimic a real industrial Historian.
- **Robust Handling**: Automated removal of "Ghost Sensors" (flatlines) and imputation of missing timestamps.

### 2. The AI Model (PyTorch Autoencoder)

- **Architecture**: A deep Undercomplete Autoencoder with Batch Normalization and Dropout.
- **Strategy**: Trained only on Normal data. The model learns the "physics" of a healthy machine. When a broken machine's data is fed in, the Reconstruction Error (MSE) spikes, flagging an anomaly.
- **Performance**:
  - **RÂ² Score (Normal Reconstruction)**: 0.76 (Strong understanding of system dynamics).
  - **ROC-AUC Score**: 0.99 (Excellent separation of Normal vs. Failure).

### 3. Business Value Dashboard

- **Financial Impact**: Calculates estimated cost savings in real-time (â‚¬150/minute of downtime saved).
- **Root Cause Analysis**: Automatically identifies which sensors are contributing most to the anomaly, helping technicians fix the right part.

---

## ğŸ› ï¸ Tech Stack

- **Language**: Python 3.x
- **Deep Learning**: PyTorch
- **Data Manipulation**: Pandas, NumPy, Scikit-Learn
- **Database**: SQLite3
- **Visualization**: Plotly, Streamlit
- **API/Web**: FastAPI, Uvicorn, HTML5, TailwindCSS

---

## âš™ï¸ Installation & Usage

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run ETL Pipeline (Database Creation)

This script reads `data/raw/sensor.csv`, cleans it, and populates `data/database.db`.

```bash
python src/etl.py
```

### 3. Train the AI Model

Trains the Autoencoder and saves the artifacts (`best_model.pth`, `scaler.joblib`) to the `models/` folder.

```bash
python notebooks/train_model.py
```

### 4. Option A: Run Streamlit Dashboard (Python Native)

```bash
streamlit run app.py
```

### 5. Option B: Run Web Dashboard (FastAPI + HTML)

#### Step 1: Start the Microservice

```bash
uvicorn src.api:app --reload
```

#### Step 2: Open `index.html` in your web browser.

---

## ğŸ“‚ Project Structure

```plaintext
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ app.py                    # Streamlit Dashboard
â”œâ”€â”€ data/                     # Data directory
â”‚   â”œâ”€â”€ database.db           # SQLite database
â”‚   â””â”€â”€ raw/                  # Raw data files
â”‚       â””â”€â”€ sensor.csv        # Raw sensor data
â”œâ”€â”€ images/                   # Images for documentation
â”œâ”€â”€ models/                   # Saved models and scalers
â”‚   â”œâ”€â”€ best_model.pth        # Trained PyTorch model
â”‚   â”œâ”€â”€ feature_columns.joblib # Feature columns
â”‚   â””â”€â”€ scaler.joblib         # Scaler for preprocessing
â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”‚   â”œâ”€â”€ EDA.ipynb             # Exploratory Data Analysis
â”‚   â””â”€â”€ train.ipynb           # Model training
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ __init__.py           # Package initialization
â”‚   â”œâ”€â”€ api.py                # FastAPI backend
â”‚   â”œâ”€â”€ etl.py                # ETL pipeline
â”‚   â”œâ”€â”€ inference.py          # Inference engine
â”œâ”€â”€ templates/                # HTML templates
â”‚   â””â”€â”€ index.html            # Web dashboard template
```

---

## ğŸ“Š Results Summary

The model successfully detects the transition from "Normal" to "Broken" hours before catastrophic failure.

| Metric      | Score | Interpretation                                      |
|-------------|-------|----------------------------------------------------|
| **ROC AUC** | 0.99  | Near perfect distinction between healthy and broken states. |
| **Precision** | Low   | Expected due to high class imbalance (safety-first approach). |
| **Recall**   | High  | The system catches the majority of failures (High Safety). |

---

**Author**: Mubeen Afzal

**For**: AWS Scheer (Studentwerk Interview)